{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an attempt to predict the 2nd term gpa given the first term gpa, math score, and high school average mark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: tensorflow in c:\\users\\k_n58\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (2.17.0)\n",
      "Requirement already satisfied: tensorflow-intel==2.17.0 in c:\\users\\k_n58\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from tensorflow) (2.17.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\k_n58\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\k_n58\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\users\\k_n58\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\k_n58\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\k_n58\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=3.10.0 in c:\\users\\k_n58\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (3.12.1)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\k_n58\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (18.1.1)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in c:\\users\\k_n58\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (0.4.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\k_n58\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (3.4.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\k_n58\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (20.9)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in c:\\users\\k_n58\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (4.25.5)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\k_n58\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (2.32.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\k_n58\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (75.1.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\k_n58\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\k_n58\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\k_n58\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\k_n58\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\k_n58\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (1.66.2)\n",
      "Requirement already satisfied: tensorboard<2.18,>=2.17 in c:\\users\\k_n58\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (2.17.1)\n",
      "Requirement already satisfied: keras>=3.2.0 in c:\\users\\k_n58\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (3.6.0)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.26.0 in c:\\users\\k_n58\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (1.26.4)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\k_n58\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.17.0->tensorflow) (0.44.0)\n",
      "Requirement already satisfied: rich in c:\\users\\k_n58\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (13.9.2)\n",
      "Requirement already satisfied: namex in c:\\users\\k_n58\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (0.0.8)\n",
      "Requirement already satisfied: optree in c:\\users\\k_n58\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (0.13.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\k_n58\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.17.0->tensorflow) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\k_n58\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.17.0->tensorflow) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\k_n58\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.17.0->tensorflow) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\k_n58\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.17.0->tensorflow) (2024.8.30)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\k_n58\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from tensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow) (3.7)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\k_n58\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from tensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\k_n58\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from tensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow) (3.0.4)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\k_n58\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from packaging->tensorflow-intel==2.17.0->tensorflow) (3.1.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\k_n58\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow) (2.1.5)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\k_n58\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from rich->keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\k_n58\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from rich->keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (2.18.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\k_n58\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (0.1.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"./data/\"\n",
    "train_file_name = \"train_data_m2.csv\"\n",
    "test_file_name = \"test_data_m2_no_transform.csv\"\n",
    "val_file_name = \"val_data_m2_no_transform.csv\"\n",
    "\n",
    "train_fullpath = os.path.join(path,train_file_name)\n",
    "test_fullpath = os.path.join(path,test_file_name)\n",
    "val_fullpath = os.path.join(path,val_file_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1st Term GPA</th>\n",
       "      <th>High School Average Mark</th>\n",
       "      <th>Math Score</th>\n",
       "      <th>2nd Term GPA</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.075965</td>\n",
       "      <td>-4.997321e-01</td>\n",
       "      <td>-2.308110</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.075965</td>\n",
       "      <td>-1.976999e+00</td>\n",
       "      <td>-1.845006</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-8.074316e-16</td>\n",
       "      <td>-0.602710</td>\n",
       "      <td>2.545455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.108072</td>\n",
       "      <td>-8.074316e-16</td>\n",
       "      <td>0.680813</td>\n",
       "      <td>3.769231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.164654</td>\n",
       "      <td>5.229910e-01</td>\n",
       "      <td>0.102971</td>\n",
       "      <td>3.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000</th>\n",
       "      <td>-0.722183</td>\n",
       "      <td>-1.522455e+00</td>\n",
       "      <td>-2.308110</td>\n",
       "      <td>2.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1001</th>\n",
       "      <td>2.226000</td>\n",
       "      <td>9.775345e-01</td>\n",
       "      <td>1.276659</td>\n",
       "      <td>4.340909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1002</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-8.074316e-16</td>\n",
       "      <td>-0.181262</td>\n",
       "      <td>2.865385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1003</th>\n",
       "      <td>-0.368401</td>\n",
       "      <td>4.093551e-01</td>\n",
       "      <td>0.079449</td>\n",
       "      <td>2.825000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1004</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-8.074316e-16</td>\n",
       "      <td>0.552844</td>\n",
       "      <td>3.454545</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1005 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      1st Term GPA  High School Average Mark  Math Score  2nd Term GPA\n",
       "0        -1.075965             -4.997321e-01   -2.308110      0.000000\n",
       "1        -1.075965             -1.976999e+00   -1.845006      0.000000\n",
       "2         0.000000             -8.074316e-16   -0.602710      2.545455\n",
       "3         2.108072             -8.074316e-16    0.680813      3.769231\n",
       "4         1.164654              5.229910e-01    0.102971      3.125000\n",
       "...            ...                       ...         ...           ...\n",
       "1000     -0.722183             -1.522455e+00   -2.308110      2.125000\n",
       "1001      2.226000              9.775345e-01    1.276659      4.340909\n",
       "1002      0.000000             -8.074316e-16   -0.181262      2.865385\n",
       "1003     -0.368401              4.093551e-01    0.079449      2.825000\n",
       "1004      0.000000             -8.074316e-16    0.552844      3.454545\n",
       "\n",
       "[1005 rows x 4 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = pd.read_csv(train_fullpath)\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1st Term GPA</th>\n",
       "      <th>High School Average Mark</th>\n",
       "      <th>Math Score</th>\n",
       "      <th>2nd Term GPA</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.075965</td>\n",
       "      <td>-4.997321e-01</td>\n",
       "      <td>-2.308110</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.075965</td>\n",
       "      <td>-1.976999e+00</td>\n",
       "      <td>-1.845006</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-8.074316e-16</td>\n",
       "      <td>-0.602710</td>\n",
       "      <td>2.545455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.108072</td>\n",
       "      <td>-8.074316e-16</td>\n",
       "      <td>0.680813</td>\n",
       "      <td>3.769231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.164654</td>\n",
       "      <td>5.229910e-01</td>\n",
       "      <td>0.102971</td>\n",
       "      <td>3.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000</th>\n",
       "      <td>-0.722183</td>\n",
       "      <td>-1.522455e+00</td>\n",
       "      <td>-2.308110</td>\n",
       "      <td>2.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1001</th>\n",
       "      <td>2.226000</td>\n",
       "      <td>9.775345e-01</td>\n",
       "      <td>1.276659</td>\n",
       "      <td>4.340909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1002</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-8.074316e-16</td>\n",
       "      <td>-0.181262</td>\n",
       "      <td>2.865385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1003</th>\n",
       "      <td>-0.368401</td>\n",
       "      <td>4.093551e-01</td>\n",
       "      <td>0.079449</td>\n",
       "      <td>2.825000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1004</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-8.074316e-16</td>\n",
       "      <td>0.552844</td>\n",
       "      <td>3.454545</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1005 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      1st Term GPA  High School Average Mark  Math Score  2nd Term GPA\n",
       "0        -1.075965             -4.997321e-01   -2.308110      0.000000\n",
       "1        -1.075965             -1.976999e+00   -1.845006      0.000000\n",
       "2         0.000000             -8.074316e-16   -0.602710      2.545455\n",
       "3         2.108072             -8.074316e-16    0.680813      3.769231\n",
       "4         1.164654              5.229910e-01    0.102971      3.125000\n",
       "...            ...                       ...         ...           ...\n",
       "1000     -0.722183             -1.522455e+00   -2.308110      2.125000\n",
       "1001      2.226000              9.775345e-01    1.276659      4.340909\n",
       "1002      0.000000             -8.074316e-16   -0.181262      2.865385\n",
       "1003     -0.368401              4.093551e-01    0.079449      2.825000\n",
       "1004      0.000000             -8.074316e-16    0.552844      3.454545\n",
       "\n",
       "[1005 rows x 4 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = pd.read_csv(train_fullpath)\n",
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1st Term GPA</th>\n",
       "      <th>High School Average Mark</th>\n",
       "      <th>Math Score</th>\n",
       "      <th>2nd Term GPA</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.075965</td>\n",
       "      <td>-4.997321e-01</td>\n",
       "      <td>-2.308110</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.075965</td>\n",
       "      <td>-1.976999e+00</td>\n",
       "      <td>-1.845006</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-8.074316e-16</td>\n",
       "      <td>-0.602710</td>\n",
       "      <td>2.545455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.108072</td>\n",
       "      <td>-8.074316e-16</td>\n",
       "      <td>0.680813</td>\n",
       "      <td>3.769231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.164654</td>\n",
       "      <td>5.229910e-01</td>\n",
       "      <td>0.102971</td>\n",
       "      <td>3.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000</th>\n",
       "      <td>-0.722183</td>\n",
       "      <td>-1.522455e+00</td>\n",
       "      <td>-2.308110</td>\n",
       "      <td>2.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1001</th>\n",
       "      <td>2.226000</td>\n",
       "      <td>9.775345e-01</td>\n",
       "      <td>1.276659</td>\n",
       "      <td>4.340909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1002</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-8.074316e-16</td>\n",
       "      <td>-0.181262</td>\n",
       "      <td>2.865385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1003</th>\n",
       "      <td>-0.368401</td>\n",
       "      <td>4.093551e-01</td>\n",
       "      <td>0.079449</td>\n",
       "      <td>2.825000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1004</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-8.074316e-16</td>\n",
       "      <td>0.552844</td>\n",
       "      <td>3.454545</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1005 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      1st Term GPA  High School Average Mark  Math Score  2nd Term GPA\n",
       "0        -1.075965             -4.997321e-01   -2.308110      0.000000\n",
       "1        -1.075965             -1.976999e+00   -1.845006      0.000000\n",
       "2         0.000000             -8.074316e-16   -0.602710      2.545455\n",
       "3         2.108072             -8.074316e-16    0.680813      3.769231\n",
       "4         1.164654              5.229910e-01    0.102971      3.125000\n",
       "...            ...                       ...         ...           ...\n",
       "1000     -0.722183             -1.522455e+00   -2.308110      2.125000\n",
       "1001      2.226000              9.775345e-01    1.276659      4.340909\n",
       "1002      0.000000             -8.074316e-16   -0.181262      2.865385\n",
       "1003     -0.368401              4.093551e-01    0.079449      2.825000\n",
       "1004      0.000000             -8.074316e-16    0.552844      3.454545\n",
       "\n",
       "[1005 rows x 4 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_data = pd.read_csv(train_fullpath)\n",
    "val_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       0.000000\n",
       "1       0.000000\n",
       "2       2.545455\n",
       "3       3.769231\n",
       "4       3.125000\n",
       "          ...   \n",
       "1000    2.125000\n",
       "1001    4.340909\n",
       "1002    2.865385\n",
       "1003    2.825000\n",
       "1004    3.454545\n",
       "Name: 2nd Term GPA, Length: 1005, dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train = train_data['2nd Term GPA']\n",
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1st Term GPA</th>\n",
       "      <th>High School Average Mark</th>\n",
       "      <th>Math Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.075965</td>\n",
       "      <td>-4.997321e-01</td>\n",
       "      <td>-2.308110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.075965</td>\n",
       "      <td>-1.976999e+00</td>\n",
       "      <td>-1.845006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-8.074316e-16</td>\n",
       "      <td>-0.602710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.108072</td>\n",
       "      <td>-8.074316e-16</td>\n",
       "      <td>0.680813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.164654</td>\n",
       "      <td>5.229910e-01</td>\n",
       "      <td>0.102971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000</th>\n",
       "      <td>-0.722183</td>\n",
       "      <td>-1.522455e+00</td>\n",
       "      <td>-2.308110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1001</th>\n",
       "      <td>2.226000</td>\n",
       "      <td>9.775345e-01</td>\n",
       "      <td>1.276659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1002</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-8.074316e-16</td>\n",
       "      <td>-0.181262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1003</th>\n",
       "      <td>-0.368401</td>\n",
       "      <td>4.093551e-01</td>\n",
       "      <td>0.079449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1004</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-8.074316e-16</td>\n",
       "      <td>0.552844</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1005 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      1st Term GPA  High School Average Mark  Math Score\n",
       "0        -1.075965             -4.997321e-01   -2.308110\n",
       "1        -1.075965             -1.976999e+00   -1.845006\n",
       "2         0.000000             -8.074316e-16   -0.602710\n",
       "3         2.108072             -8.074316e-16    0.680813\n",
       "4         1.164654              5.229910e-01    0.102971\n",
       "...            ...                       ...         ...\n",
       "1000     -0.722183             -1.522455e+00   -2.308110\n",
       "1001      2.226000              9.775345e-01    1.276659\n",
       "1002      0.000000             -8.074316e-16   -0.181262\n",
       "1003     -0.368401              4.093551e-01    0.079449\n",
       "1004      0.000000             -8.074316e-16    0.552844\n",
       "\n",
       "[1005 rows x 3 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = train_data.copy()\n",
    "X_train = X_train.drop('2nd Term GPA',axis=1)\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       0.000000\n",
       "1       0.000000\n",
       "2       2.545455\n",
       "3       3.769231\n",
       "4       3.125000\n",
       "          ...   \n",
       "1000    2.125000\n",
       "1001    4.340909\n",
       "1002    2.865385\n",
       "1003    2.825000\n",
       "1004    3.454545\n",
       "Name: 2nd Term GPA, Length: 1005, dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test =  test_data['2nd Term GPA']\n",
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1st Term GPA</th>\n",
       "      <th>High School Average Mark</th>\n",
       "      <th>Math Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.075965</td>\n",
       "      <td>-4.997321e-01</td>\n",
       "      <td>-2.308110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.075965</td>\n",
       "      <td>-1.976999e+00</td>\n",
       "      <td>-1.845006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-8.074316e-16</td>\n",
       "      <td>-0.602710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.108072</td>\n",
       "      <td>-8.074316e-16</td>\n",
       "      <td>0.680813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.164654</td>\n",
       "      <td>5.229910e-01</td>\n",
       "      <td>0.102971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000</th>\n",
       "      <td>-0.722183</td>\n",
       "      <td>-1.522455e+00</td>\n",
       "      <td>-2.308110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1001</th>\n",
       "      <td>2.226000</td>\n",
       "      <td>9.775345e-01</td>\n",
       "      <td>1.276659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1002</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-8.074316e-16</td>\n",
       "      <td>-0.181262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1003</th>\n",
       "      <td>-0.368401</td>\n",
       "      <td>4.093551e-01</td>\n",
       "      <td>0.079449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1004</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-8.074316e-16</td>\n",
       "      <td>0.552844</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1005 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      1st Term GPA  High School Average Mark  Math Score\n",
       "0        -1.075965             -4.997321e-01   -2.308110\n",
       "1        -1.075965             -1.976999e+00   -1.845006\n",
       "2         0.000000             -8.074316e-16   -0.602710\n",
       "3         2.108072             -8.074316e-16    0.680813\n",
       "4         1.164654              5.229910e-01    0.102971\n",
       "...            ...                       ...         ...\n",
       "1000     -0.722183             -1.522455e+00   -2.308110\n",
       "1001      2.226000              9.775345e-01    1.276659\n",
       "1002      0.000000             -8.074316e-16   -0.181262\n",
       "1003     -0.368401              4.093551e-01    0.079449\n",
       "1004      0.000000             -8.074316e-16    0.552844\n",
       "\n",
       "[1005 rows x 3 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test = test_data.copy()\n",
    "X_test = X_test.drop('2nd Term GPA',axis=1)\n",
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       0.000000\n",
       "1       0.000000\n",
       "2       2.545455\n",
       "3       3.769231\n",
       "4       3.125000\n",
       "          ...   \n",
       "1000    2.125000\n",
       "1001    4.340909\n",
       "1002    2.865385\n",
       "1003    2.825000\n",
       "1004    3.454545\n",
       "Name: 2nd Term GPA, Length: 1005, dtype: float64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_val =  test_data['2nd Term GPA']\n",
    "y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1st Term GPA</th>\n",
       "      <th>High School Average Mark</th>\n",
       "      <th>Math Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.075965</td>\n",
       "      <td>-4.997321e-01</td>\n",
       "      <td>-2.308110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.075965</td>\n",
       "      <td>-1.976999e+00</td>\n",
       "      <td>-1.845006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-8.074316e-16</td>\n",
       "      <td>-0.602710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.108072</td>\n",
       "      <td>-8.074316e-16</td>\n",
       "      <td>0.680813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.164654</td>\n",
       "      <td>5.229910e-01</td>\n",
       "      <td>0.102971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000</th>\n",
       "      <td>-0.722183</td>\n",
       "      <td>-1.522455e+00</td>\n",
       "      <td>-2.308110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1001</th>\n",
       "      <td>2.226000</td>\n",
       "      <td>9.775345e-01</td>\n",
       "      <td>1.276659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1002</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-8.074316e-16</td>\n",
       "      <td>-0.181262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1003</th>\n",
       "      <td>-0.368401</td>\n",
       "      <td>4.093551e-01</td>\n",
       "      <td>0.079449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1004</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-8.074316e-16</td>\n",
       "      <td>0.552844</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1005 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      1st Term GPA  High School Average Mark  Math Score\n",
       "0        -1.075965             -4.997321e-01   -2.308110\n",
       "1        -1.075965             -1.976999e+00   -1.845006\n",
       "2         0.000000             -8.074316e-16   -0.602710\n",
       "3         2.108072             -8.074316e-16    0.680813\n",
       "4         1.164654              5.229910e-01    0.102971\n",
       "...            ...                       ...         ...\n",
       "1000     -0.722183             -1.522455e+00   -2.308110\n",
       "1001      2.226000              9.775345e-01    1.276659\n",
       "1002      0.000000             -8.074316e-16   -0.181262\n",
       "1003     -0.368401              4.093551e-01    0.079449\n",
       "1004      0.000000             -8.074316e-16    0.552844\n",
       "\n",
       "[1005 rows x 3 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_val = val_data.copy()\n",
    "X_val = X_val.drop('2nd Term GPA',axis=1)\n",
    "X_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3,)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_dim  = (X_train.shape[1],)\n",
    "input_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import models, layers\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\K_N58\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Dense(128, activation='relu',input_shape=input_dim))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Dropout(0.25))\n",
    "model.add(layers.Dense(256, activation='relu'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Dropout(0.25))\n",
    "model.add(layers.Dense(128, activation='relu'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Dropout(0.25))\n",
    "model.add(layers.Dense(1, activation='linear'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">33,024</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_1           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_2           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">129</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │           \u001b[38;5;34m512\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │           \u001b[38;5;34m512\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │        \u001b[38;5;34m33,024\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_1           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │         \u001b[38;5;34m1,024\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │        \u001b[38;5;34m32,896\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_2           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │           \u001b[38;5;34m512\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │           \u001b[38;5;34m129\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">68,609</span> (268.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m68,609\u001b[0m (268.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">67,585</span> (264.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m67,585\u001b[0m (264.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> (4.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m1,024\u001b[0m (4.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks\n",
    "es = EarlyStopping(monitor=\"val_loss\",\n",
    "                   patience=25,\n",
    "                   verbose=1,\n",
    "                   restore_best_weights=True\n",
    "                   )\n",
    "\n",
    "mc = ModelCheckpoint(\"best_model_2.keras\",\n",
    "                     monitor=\"val_accuracy\",\n",
    "                     verbose=1,\n",
    "                     save_best_only=True,\n",
    "                     mode=\"max\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='mse',\n",
    "              metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 9.2204 - mae: 2.6077 - val_loss: 6.1984 - val_mae: 2.1969\n",
      "Epoch 2/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 5.5676 - mae: 2.0612 - val_loss: 4.6207 - val_mae: 1.9049\n",
      "Epoch 3/1000\n",
      "\u001b[1m 1/32\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 4.9571 - mae: 2.0378"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\K_N58\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\keras\\src\\callbacks\\model_checkpoint.py:206: UserWarning: Can save best model only with val_accuracy available, skipping.\n",
      "  self._save_model(epoch=epoch, batch=None, logs=logs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.4427 - mae: 1.8094 - val_loss: 3.1953 - val_mae: 1.5993\n",
      "Epoch 4/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.1458 - mae: 1.4930 - val_loss: 2.6562 - val_mae: 1.4603\n",
      "Epoch 5/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.3538 - mae: 1.2329 - val_loss: 1.5768 - val_mae: 1.1231\n",
      "Epoch 6/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.8054 - mae: 1.0717 - val_loss: 1.3875 - val_mae: 1.0545\n",
      "Epoch 7/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.7791 - mae: 1.0476 - val_loss: 0.9975 - val_mae: 0.8564\n",
      "Epoch 8/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.4364 - mae: 0.9209 - val_loss: 0.8342 - val_mae: 0.7518\n",
      "Epoch 9/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.4009 - mae: 0.9366 - val_loss: 0.7592 - val_mae: 0.7038\n",
      "Epoch 10/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.4173 - mae: 0.9272 - val_loss: 0.9076 - val_mae: 0.7907\n",
      "Epoch 11/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.3288 - mae: 0.9010 - val_loss: 0.7258 - val_mae: 0.6869\n",
      "Epoch 12/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.2570 - mae: 0.8514 - val_loss: 0.7406 - val_mae: 0.6847\n",
      "Epoch 13/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.3512 - mae: 0.8861 - val_loss: 0.6890 - val_mae: 0.6324\n",
      "Epoch 14/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.1260 - mae: 0.8189 - val_loss: 0.7720 - val_mae: 0.7246\n",
      "Epoch 15/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.1085 - mae: 0.8145 - val_loss: 0.6462 - val_mae: 0.5904\n",
      "Epoch 16/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.1723 - mae: 0.8186 - val_loss: 0.6717 - val_mae: 0.6044\n",
      "Epoch 17/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.2638 - mae: 0.8452 - val_loss: 0.6345 - val_mae: 0.5647\n",
      "Epoch 18/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.3480 - mae: 0.8680 - val_loss: 0.6403 - val_mae: 0.5727\n",
      "Epoch 19/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.1132 - mae: 0.7900 - val_loss: 0.6222 - val_mae: 0.5653\n",
      "Epoch 20/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.0658 - mae: 0.7638 - val_loss: 0.6094 - val_mae: 0.5617\n",
      "Epoch 21/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.9457 - mae: 0.7350 - val_loss: 0.6194 - val_mae: 0.5715\n",
      "Epoch 22/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.1182 - mae: 0.8189 - val_loss: 0.6134 - val_mae: 0.5588\n",
      "Epoch 23/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.9983 - mae: 0.7651 - val_loss: 0.6219 - val_mae: 0.5589\n",
      "Epoch 24/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.9893 - mae: 0.7645 - val_loss: 0.6338 - val_mae: 0.5764\n",
      "Epoch 25/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.9581 - mae: 0.7318 - val_loss: 0.5922 - val_mae: 0.5458\n",
      "Epoch 26/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.0248 - mae: 0.7528 - val_loss: 0.6259 - val_mae: 0.5461\n",
      "Epoch 27/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.9117 - mae: 0.7146 - val_loss: 0.6211 - val_mae: 0.5376\n",
      "Epoch 28/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.0022 - mae: 0.7441 - val_loss: 0.6043 - val_mae: 0.5324\n",
      "Epoch 29/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.8913 - mae: 0.7281 - val_loss: 0.5908 - val_mae: 0.5368\n",
      "Epoch 30/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.8042 - mae: 0.6766 - val_loss: 0.6196 - val_mae: 0.5642\n",
      "Epoch 31/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.8659 - mae: 0.7067 - val_loss: 0.6085 - val_mae: 0.5339\n",
      "Epoch 32/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.8550 - mae: 0.7113 - val_loss: 0.5842 - val_mae: 0.5299\n",
      "Epoch 33/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.8938 - mae: 0.7218 - val_loss: 0.5940 - val_mae: 0.5452\n",
      "Epoch 34/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.9183 - mae: 0.7207 - val_loss: 0.5882 - val_mae: 0.5455\n",
      "Epoch 35/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.8406 - mae: 0.6802 - val_loss: 0.6052 - val_mae: 0.5574\n",
      "Epoch 36/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.8821 - mae: 0.7127 - val_loss: 0.5870 - val_mae: 0.5258\n",
      "Epoch 37/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.9595 - mae: 0.7387 - val_loss: 0.5917 - val_mae: 0.5268\n",
      "Epoch 38/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.9182 - mae: 0.7316 - val_loss: 0.6039 - val_mae: 0.5465\n",
      "Epoch 39/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.8981 - mae: 0.7216 - val_loss: 0.5954 - val_mae: 0.5437\n",
      "Epoch 40/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.8006 - mae: 0.6745 - val_loss: 0.5967 - val_mae: 0.5566\n",
      "Epoch 41/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.9156 - mae: 0.7339 - val_loss: 0.6008 - val_mae: 0.5491\n",
      "Epoch 42/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.8288 - mae: 0.6877 - val_loss: 0.5852 - val_mae: 0.5389\n",
      "Epoch 43/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.8216 - mae: 0.6812 - val_loss: 0.5816 - val_mae: 0.5252\n",
      "Epoch 44/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.8217 - mae: 0.6756 - val_loss: 0.5972 - val_mae: 0.5730\n",
      "Epoch 45/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.7455 - mae: 0.6564 - val_loss: 0.5782 - val_mae: 0.5241\n",
      "Epoch 46/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.8689 - mae: 0.7024 - val_loss: 0.5869 - val_mae: 0.5245\n",
      "Epoch 47/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.7614 - mae: 0.6491 - val_loss: 0.5852 - val_mae: 0.5357\n",
      "Epoch 48/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.8354 - mae: 0.6633 - val_loss: 0.5783 - val_mae: 0.5338\n",
      "Epoch 49/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.8669 - mae: 0.6877 - val_loss: 0.5794 - val_mae: 0.5351\n",
      "Epoch 50/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.8082 - mae: 0.6781 - val_loss: 0.5865 - val_mae: 0.5252\n",
      "Epoch 51/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.8740 - mae: 0.6735 - val_loss: 0.5968 - val_mae: 0.5326\n",
      "Epoch 52/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.7387 - mae: 0.6286 - val_loss: 0.5851 - val_mae: 0.5512\n",
      "Epoch 53/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7919 - mae: 0.6673 - val_loss: 0.5785 - val_mae: 0.5333\n",
      "Epoch 54/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7821 - mae: 0.6578 - val_loss: 0.5804 - val_mae: 0.5323\n",
      "Epoch 55/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.8042 - mae: 0.6757 - val_loss: 0.5764 - val_mae: 0.5336\n",
      "Epoch 56/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7292 - mae: 0.6486 - val_loss: 0.5775 - val_mae: 0.5332\n",
      "Epoch 57/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.7154 - mae: 0.6239 - val_loss: 0.5829 - val_mae: 0.5478\n",
      "Epoch 58/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.8179 - mae: 0.6737 - val_loss: 0.5813 - val_mae: 0.5231\n",
      "Epoch 59/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.8646 - mae: 0.7147 - val_loss: 0.6008 - val_mae: 0.5776\n",
      "Epoch 60/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.8347 - mae: 0.6904 - val_loss: 0.5812 - val_mae: 0.5223\n",
      "Epoch 61/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7758 - mae: 0.6337 - val_loss: 0.5755 - val_mae: 0.5399\n",
      "Epoch 62/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7198 - mae: 0.6294 - val_loss: 0.5854 - val_mae: 0.5590\n",
      "Epoch 63/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.8284 - mae: 0.6649 - val_loss: 0.5729 - val_mae: 0.5299\n",
      "Epoch 64/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.6932 - mae: 0.6247 - val_loss: 0.5748 - val_mae: 0.5216\n",
      "Epoch 65/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.8293 - mae: 0.6718 - val_loss: 0.5747 - val_mae: 0.5329\n",
      "Epoch 66/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.7559 - mae: 0.6452 - val_loss: 0.5793 - val_mae: 0.5212\n",
      "Epoch 67/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.6979 - mae: 0.6204 - val_loss: 0.5863 - val_mae: 0.5563\n",
      "Epoch 68/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.7972 - mae: 0.6640 - val_loss: 0.5811 - val_mae: 0.5455\n",
      "Epoch 69/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.7676 - mae: 0.6693 - val_loss: 0.5830 - val_mae: 0.5531\n",
      "Epoch 70/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.6921 - mae: 0.6155 - val_loss: 0.5788 - val_mae: 0.5182\n",
      "Epoch 71/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.7809 - mae: 0.6559 - val_loss: 0.5741 - val_mae: 0.5256\n",
      "Epoch 72/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.7180 - mae: 0.6190 - val_loss: 0.5772 - val_mae: 0.5357\n",
      "Epoch 73/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.6950 - mae: 0.6392 - val_loss: 0.5959 - val_mae: 0.5724\n",
      "Epoch 74/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.7623 - mae: 0.6651 - val_loss: 0.5765 - val_mae: 0.5206\n",
      "Epoch 75/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.7850 - mae: 0.6573 - val_loss: 0.5717 - val_mae: 0.5110\n",
      "Epoch 76/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.6561 - mae: 0.5930 - val_loss: 0.5752 - val_mae: 0.5130\n",
      "Epoch 77/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.7730 - mae: 0.6376 - val_loss: 0.5786 - val_mae: 0.5332\n",
      "Epoch 78/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.6846 - mae: 0.6311 - val_loss: 0.5758 - val_mae: 0.5440\n",
      "Epoch 79/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.7491 - mae: 0.6597 - val_loss: 0.5775 - val_mae: 0.5336\n",
      "Epoch 80/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.8067 - mae: 0.6692 - val_loss: 0.5762 - val_mae: 0.5210\n",
      "Epoch 81/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.7214 - mae: 0.6422 - val_loss: 0.5683 - val_mae: 0.5250\n",
      "Epoch 82/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.7114 - mae: 0.6183 - val_loss: 0.5789 - val_mae: 0.5527\n",
      "Epoch 83/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.8321 - mae: 0.6902 - val_loss: 0.5748 - val_mae: 0.5201\n",
      "Epoch 84/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7089 - mae: 0.6312 - val_loss: 0.5690 - val_mae: 0.5218\n",
      "Epoch 85/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.7972 - mae: 0.6711 - val_loss: 0.5740 - val_mae: 0.5288\n",
      "Epoch 86/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.7243 - mae: 0.6479 - val_loss: 0.5874 - val_mae: 0.5183\n",
      "Epoch 87/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7268 - mae: 0.6252 - val_loss: 0.5681 - val_mae: 0.5131\n",
      "Epoch 88/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7677 - mae: 0.6431 - val_loss: 0.5831 - val_mae: 0.5146\n",
      "Epoch 89/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.7567 - mae: 0.6591 - val_loss: 0.5701 - val_mae: 0.5314\n",
      "Epoch 90/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7077 - mae: 0.6128 - val_loss: 0.5797 - val_mae: 0.5396\n",
      "Epoch 91/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.7198 - mae: 0.6363 - val_loss: 0.5822 - val_mae: 0.5599\n",
      "Epoch 92/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.6428 - mae: 0.6127 - val_loss: 0.5656 - val_mae: 0.5171\n",
      "Epoch 93/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.7655 - mae: 0.6310 - val_loss: 0.5702 - val_mae: 0.5375\n",
      "Epoch 94/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.6765 - mae: 0.6273 - val_loss: 0.5679 - val_mae: 0.5189\n",
      "Epoch 95/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.6819 - mae: 0.6167 - val_loss: 0.5685 - val_mae: 0.5282\n",
      "Epoch 96/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.7253 - mae: 0.6406 - val_loss: 0.5655 - val_mae: 0.5278\n",
      "Epoch 97/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7389 - mae: 0.6323 - val_loss: 0.5656 - val_mae: 0.5342\n",
      "Epoch 98/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.6647 - mae: 0.6122 - val_loss: 0.5661 - val_mae: 0.5191\n",
      "Epoch 99/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7286 - mae: 0.6173 - val_loss: 0.5659 - val_mae: 0.5324\n",
      "Epoch 100/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.6263 - mae: 0.5805 - val_loss: 0.5735 - val_mae: 0.5505\n",
      "Epoch 101/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.6526 - mae: 0.6106 - val_loss: 0.5728 - val_mae: 0.5460\n",
      "Epoch 102/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.7867 - mae: 0.6700 - val_loss: 0.5684 - val_mae: 0.5088\n",
      "Epoch 103/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.6994 - mae: 0.6139 - val_loss: 0.5647 - val_mae: 0.5201\n",
      "Epoch 104/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.8694 - mae: 0.6868 - val_loss: 0.5611 - val_mae: 0.5168\n",
      "Epoch 105/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7921 - mae: 0.6587 - val_loss: 0.5705 - val_mae: 0.5078\n",
      "Epoch 106/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7132 - mae: 0.6102 - val_loss: 0.5715 - val_mae: 0.5378\n",
      "Epoch 107/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.6655 - mae: 0.5994 - val_loss: 0.5680 - val_mae: 0.5430\n",
      "Epoch 108/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7047 - mae: 0.6310 - val_loss: 0.5651 - val_mae: 0.5202\n",
      "Epoch 109/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7370 - mae: 0.6248 - val_loss: 0.5662 - val_mae: 0.5217\n",
      "Epoch 110/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7528 - mae: 0.6437 - val_loss: 0.5615 - val_mae: 0.5097\n",
      "Epoch 111/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.6639 - mae: 0.5853 - val_loss: 0.5794 - val_mae: 0.5603\n",
      "Epoch 112/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.8613 - mae: 0.6965 - val_loss: 0.5606 - val_mae: 0.5067\n",
      "Epoch 113/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.6633 - mae: 0.5936 - val_loss: 0.5675 - val_mae: 0.5121\n",
      "Epoch 114/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7047 - mae: 0.6087 - val_loss: 0.5607 - val_mae: 0.5089\n",
      "Epoch 115/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.6934 - mae: 0.5944 - val_loss: 0.5674 - val_mae: 0.5102\n",
      "Epoch 116/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7135 - mae: 0.6210 - val_loss: 0.5678 - val_mae: 0.5111\n",
      "Epoch 117/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7280 - mae: 0.6272 - val_loss: 0.5732 - val_mae: 0.5075\n",
      "Epoch 118/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.6912 - mae: 0.6030 - val_loss: 0.5678 - val_mae: 0.5115\n",
      "Epoch 119/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7759 - mae: 0.6387 - val_loss: 0.5644 - val_mae: 0.5104\n",
      "Epoch 120/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7544 - mae: 0.6247 - val_loss: 0.5653 - val_mae: 0.5274\n",
      "Epoch 121/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7104 - mae: 0.6313 - val_loss: 0.5592 - val_mae: 0.5147\n",
      "Epoch 122/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7153 - mae: 0.6104 - val_loss: 0.5625 - val_mae: 0.5264\n",
      "Epoch 123/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7017 - mae: 0.6205 - val_loss: 0.5641 - val_mae: 0.5087\n",
      "Epoch 124/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7232 - mae: 0.6104 - val_loss: 0.5644 - val_mae: 0.5416\n",
      "Epoch 125/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.6868 - mae: 0.6278 - val_loss: 0.5688 - val_mae: 0.5502\n",
      "Epoch 126/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.6975 - mae: 0.6302 - val_loss: 0.5650 - val_mae: 0.5198\n",
      "Epoch 127/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.6597 - mae: 0.6107 - val_loss: 0.5609 - val_mae: 0.5059\n",
      "Epoch 128/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7194 - mae: 0.6194 - val_loss: 0.5700 - val_mae: 0.5534\n",
      "Epoch 129/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7647 - mae: 0.6487 - val_loss: 0.5670 - val_mae: 0.5243\n",
      "Epoch 130/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.7190 - mae: 0.6174 - val_loss: 0.5583 - val_mae: 0.5145\n",
      "Epoch 131/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.6598 - mae: 0.5860 - val_loss: 0.5591 - val_mae: 0.5271\n",
      "Epoch 132/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.6769 - mae: 0.6092 - val_loss: 0.5621 - val_mae: 0.5202\n",
      "Epoch 133/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.6912 - mae: 0.5965 - val_loss: 0.5663 - val_mae: 0.5353\n",
      "Epoch 134/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7202 - mae: 0.6311 - val_loss: 0.5563 - val_mae: 0.5221\n",
      "Epoch 135/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.6581 - mae: 0.5975 - val_loss: 0.5538 - val_mae: 0.5178\n",
      "Epoch 136/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.6557 - mae: 0.5826 - val_loss: 0.5627 - val_mae: 0.5282\n",
      "Epoch 137/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.6707 - mae: 0.6089 - val_loss: 0.5774 - val_mae: 0.5641\n",
      "Epoch 138/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7664 - mae: 0.6521 - val_loss: 0.5654 - val_mae: 0.5275\n",
      "Epoch 139/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.6764 - mae: 0.6112 - val_loss: 0.5614 - val_mae: 0.5072\n",
      "Epoch 140/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.6755 - mae: 0.5854 - val_loss: 0.5599 - val_mae: 0.5241\n",
      "Epoch 141/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.6623 - mae: 0.5934 - val_loss: 0.5551 - val_mae: 0.5104\n",
      "Epoch 142/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.6772 - mae: 0.5899 - val_loss: 0.5640 - val_mae: 0.5458\n",
      "Epoch 143/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.6330 - mae: 0.5823 - val_loss: 0.5739 - val_mae: 0.5616\n",
      "Epoch 144/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.6288 - mae: 0.5936 - val_loss: 0.5532 - val_mae: 0.5117\n",
      "Epoch 145/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.6895 - mae: 0.6079 - val_loss: 0.5653 - val_mae: 0.5485\n",
      "Epoch 146/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.6575 - mae: 0.6118 - val_loss: 0.5592 - val_mae: 0.5328\n",
      "Epoch 147/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.6443 - mae: 0.5823 - val_loss: 0.5587 - val_mae: 0.5383\n",
      "Epoch 148/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7240 - mae: 0.6283 - val_loss: 0.5558 - val_mae: 0.5090\n",
      "Epoch 149/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5734 - mae: 0.5609 - val_loss: 0.5650 - val_mae: 0.5443\n",
      "Epoch 150/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7295 - mae: 0.6249 - val_loss: 0.5608 - val_mae: 0.4963\n",
      "Epoch 151/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.6959 - mae: 0.6268 - val_loss: 0.5540 - val_mae: 0.5139\n",
      "Epoch 152/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7237 - mae: 0.6246 - val_loss: 0.5539 - val_mae: 0.5119\n",
      "Epoch 153/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.6984 - mae: 0.6051 - val_loss: 0.5549 - val_mae: 0.5192\n",
      "Epoch 154/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.6325 - mae: 0.5823 - val_loss: 0.5624 - val_mae: 0.5042\n",
      "Epoch 155/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5900 - mae: 0.5632 - val_loss: 0.5639 - val_mae: 0.5435\n",
      "Epoch 156/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.7047 - mae: 0.6123 - val_loss: 0.5566 - val_mae: 0.4974\n",
      "Epoch 157/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.6163 - mae: 0.5717 - val_loss: 0.5527 - val_mae: 0.5228\n",
      "Epoch 158/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7513 - mae: 0.6395 - val_loss: 0.5518 - val_mae: 0.5184\n",
      "Epoch 159/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7824 - mae: 0.6592 - val_loss: 0.5642 - val_mae: 0.5495\n",
      "Epoch 160/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7416 - mae: 0.6327 - val_loss: 0.5539 - val_mae: 0.5264\n",
      "Epoch 161/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.6107 - mae: 0.5749 - val_loss: 0.5601 - val_mae: 0.5388\n",
      "Epoch 162/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.6869 - mae: 0.6300 - val_loss: 0.5715 - val_mae: 0.5585\n",
      "Epoch 163/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.6398 - mae: 0.5885 - val_loss: 0.5591 - val_mae: 0.5011\n",
      "Epoch 164/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.6726 - mae: 0.6172 - val_loss: 0.5537 - val_mae: 0.5188\n",
      "Epoch 165/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.6149 - mae: 0.5868 - val_loss: 0.5585 - val_mae: 0.5290\n",
      "Epoch 166/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7457 - mae: 0.6254 - val_loss: 0.5481 - val_mae: 0.5061\n",
      "Epoch 167/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.6284 - mae: 0.5686 - val_loss: 0.5538 - val_mae: 0.4960\n",
      "Epoch 168/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.6909 - mae: 0.6013 - val_loss: 0.5653 - val_mae: 0.5330\n",
      "Epoch 169/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.6157 - mae: 0.5859 - val_loss: 0.5576 - val_mae: 0.5431\n",
      "Epoch 170/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.6865 - mae: 0.6343 - val_loss: 0.5612 - val_mae: 0.5509\n",
      "Epoch 171/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.6400 - mae: 0.6094 - val_loss: 0.5716 - val_mae: 0.5575\n",
      "Epoch 172/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.6949 - mae: 0.6233 - val_loss: 0.5551 - val_mae: 0.5365\n",
      "Epoch 173/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.5659 - mae: 0.5645 - val_loss: 0.5704 - val_mae: 0.5658\n",
      "Epoch 174/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.6604 - mae: 0.6192 - val_loss: 0.5483 - val_mae: 0.5085\n",
      "Epoch 175/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.7158 - mae: 0.6094 - val_loss: 0.5489 - val_mae: 0.5146\n",
      "Epoch 176/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.6718 - mae: 0.5979 - val_loss: 0.5481 - val_mae: 0.5205\n",
      "Epoch 177/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.6375 - mae: 0.5828 - val_loss: 0.5802 - val_mae: 0.5681\n",
      "Epoch 178/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.6955 - mae: 0.6122 - val_loss: 0.5529 - val_mae: 0.5218\n",
      "Epoch 179/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.5681 - mae: 0.5629 - val_loss: 0.5615 - val_mae: 0.5482\n",
      "Epoch 180/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.5996 - mae: 0.5831 - val_loss: 0.5484 - val_mae: 0.5261\n",
      "Epoch 181/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.8378 - mae: 0.6739 - val_loss: 0.5474 - val_mae: 0.4968\n",
      "Epoch 182/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7027 - mae: 0.5945 - val_loss: 0.5524 - val_mae: 0.5359\n",
      "Epoch 183/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.6917 - mae: 0.6077 - val_loss: 0.5500 - val_mae: 0.5068\n",
      "Epoch 184/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.6300 - mae: 0.5985 - val_loss: 0.5495 - val_mae: 0.5257\n",
      "Epoch 185/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.6624 - mae: 0.6132 - val_loss: 0.5478 - val_mae: 0.5227\n",
      "Epoch 186/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5889 - mae: 0.5691 - val_loss: 0.5432 - val_mae: 0.5131\n",
      "Epoch 187/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.6939 - mae: 0.6128 - val_loss: 0.5459 - val_mae: 0.4996\n",
      "Epoch 188/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.6787 - mae: 0.6129 - val_loss: 0.5439 - val_mae: 0.5092\n",
      "Epoch 189/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.6818 - mae: 0.6052 - val_loss: 0.5458 - val_mae: 0.5001\n",
      "Epoch 190/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7086 - mae: 0.6073 - val_loss: 0.5491 - val_mae: 0.5107\n",
      "Epoch 191/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.6354 - mae: 0.5875 - val_loss: 0.5453 - val_mae: 0.5096\n",
      "Epoch 192/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.6400 - mae: 0.5709 - val_loss: 0.5505 - val_mae: 0.5252\n",
      "Epoch 193/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.6540 - mae: 0.5956 - val_loss: 0.5461 - val_mae: 0.5190\n",
      "Epoch 194/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.6772 - mae: 0.5974 - val_loss: 0.5521 - val_mae: 0.5000\n",
      "Epoch 195/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.6968 - mae: 0.5886 - val_loss: 0.5498 - val_mae: 0.4951\n",
      "Epoch 196/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.6822 - mae: 0.5986 - val_loss: 0.5528 - val_mae: 0.5230\n",
      "Epoch 197/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.6233 - mae: 0.5951 - val_loss: 0.5426 - val_mae: 0.5093\n",
      "Epoch 198/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.6526 - mae: 0.5902 - val_loss: 0.5493 - val_mae: 0.5290\n",
      "Epoch 199/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.6631 - mae: 0.6073 - val_loss: 0.5437 - val_mae: 0.5042\n",
      "Epoch 200/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.6575 - mae: 0.5873 - val_loss: 0.5591 - val_mae: 0.5420\n",
      "Epoch 201/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.6798 - mae: 0.6065 - val_loss: 0.5546 - val_mae: 0.5374\n",
      "Epoch 202/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.6471 - mae: 0.6024 - val_loss: 0.5610 - val_mae: 0.5427\n",
      "Epoch 203/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.6558 - mae: 0.6006 - val_loss: 0.5773 - val_mae: 0.5710\n",
      "Epoch 204/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.5915 - mae: 0.5804 - val_loss: 0.5482 - val_mae: 0.5343\n",
      "Epoch 205/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.6562 - mae: 0.6056 - val_loss: 0.5484 - val_mae: 0.4926\n",
      "Epoch 206/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.6915 - mae: 0.5939 - val_loss: 0.5444 - val_mae: 0.4913\n",
      "Epoch 207/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.6013 - mae: 0.5571 - val_loss: 0.5578 - val_mae: 0.5505\n",
      "Epoch 208/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.6342 - mae: 0.6106 - val_loss: 0.5548 - val_mae: 0.5451\n",
      "Epoch 209/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.6152 - mae: 0.5691 - val_loss: 0.5523 - val_mae: 0.5232\n",
      "Epoch 210/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.6225 - mae: 0.5789 - val_loss: 0.5374 - val_mae: 0.5087\n",
      "Epoch 211/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.6408 - mae: 0.5738 - val_loss: 0.5383 - val_mae: 0.4971\n",
      "Epoch 212/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7828 - mae: 0.6386 - val_loss: 0.5454 - val_mae: 0.4930\n",
      "Epoch 213/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.6617 - mae: 0.6004 - val_loss: 0.5372 - val_mae: 0.5085\n",
      "Epoch 214/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7464 - mae: 0.6244 - val_loss: 0.5445 - val_mae: 0.5106\n",
      "Epoch 215/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.6574 - mae: 0.6097 - val_loss: 0.5540 - val_mae: 0.4963\n",
      "Epoch 216/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7200 - mae: 0.6115 - val_loss: 0.5382 - val_mae: 0.4975\n",
      "Epoch 217/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.6898 - mae: 0.6135 - val_loss: 0.5352 - val_mae: 0.5070\n",
      "Epoch 218/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7066 - mae: 0.5994 - val_loss: 0.5384 - val_mae: 0.5040\n",
      "Epoch 219/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.6383 - mae: 0.5882 - val_loss: 0.5440 - val_mae: 0.5213\n",
      "Epoch 220/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.6393 - mae: 0.5795 - val_loss: 0.5386 - val_mae: 0.4974\n",
      "Epoch 221/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.6833 - mae: 0.6027 - val_loss: 0.5355 - val_mae: 0.5002\n",
      "Epoch 222/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.7017 - mae: 0.6103 - val_loss: 0.5443 - val_mae: 0.5237\n",
      "Epoch 223/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.6019 - mae: 0.5844 - val_loss: 0.5722 - val_mae: 0.5702\n",
      "Epoch 224/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.6342 - mae: 0.5843 - val_loss: 0.5562 - val_mae: 0.5561\n",
      "Epoch 225/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.6324 - mae: 0.5935 - val_loss: 0.5431 - val_mae: 0.5247\n",
      "Epoch 226/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.6549 - mae: 0.5985 - val_loss: 0.5353 - val_mae: 0.5004\n",
      "Epoch 227/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.7170 - mae: 0.5954 - val_loss: 0.5383 - val_mae: 0.5085\n",
      "Epoch 228/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.6038 - mae: 0.5596 - val_loss: 0.5654 - val_mae: 0.5684\n",
      "Epoch 229/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.6578 - mae: 0.6010 - val_loss: 0.5475 - val_mae: 0.5417\n",
      "Epoch 230/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.6254 - mae: 0.5885 - val_loss: 0.5347 - val_mae: 0.5041\n",
      "Epoch 231/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7511 - mae: 0.6367 - val_loss: 0.5317 - val_mae: 0.5134\n",
      "Epoch 232/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.6318 - mae: 0.5818 - val_loss: 0.5329 - val_mae: 0.5007\n",
      "Epoch 233/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.7250 - mae: 0.6351 - val_loss: 0.5371 - val_mae: 0.4944\n",
      "Epoch 234/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.6286 - mae: 0.5752 - val_loss: 0.5444 - val_mae: 0.5381\n",
      "Epoch 235/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.6803 - mae: 0.6100 - val_loss: 0.5360 - val_mae: 0.4878\n",
      "Epoch 236/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.5979 - mae: 0.5471 - val_loss: 0.5274 - val_mae: 0.4966\n",
      "Epoch 237/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.6437 - mae: 0.5769 - val_loss: 0.5386 - val_mae: 0.5269\n",
      "Epoch 238/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.5450 - mae: 0.5635 - val_loss: 0.5399 - val_mae: 0.5343\n",
      "Epoch 239/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.6982 - mae: 0.6118 - val_loss: 0.5393 - val_mae: 0.5294\n",
      "Epoch 240/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.5872 - mae: 0.5614 - val_loss: 0.5348 - val_mae: 0.5144\n",
      "Epoch 241/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7372 - mae: 0.6393 - val_loss: 0.5343 - val_mae: 0.4838\n",
      "Epoch 242/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7256 - mae: 0.5939 - val_loss: 0.5340 - val_mae: 0.4930\n",
      "Epoch 243/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.6081 - mae: 0.5645 - val_loss: 0.5399 - val_mae: 0.5243\n",
      "Epoch 244/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.7476 - mae: 0.6148 - val_loss: 0.5301 - val_mae: 0.4931\n",
      "Epoch 245/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.6221 - mae: 0.5586 - val_loss: 0.5317 - val_mae: 0.5176\n",
      "Epoch 246/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.6187 - mae: 0.5714 - val_loss: 0.5289 - val_mae: 0.5070\n",
      "Epoch 247/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.6227 - mae: 0.5701 - val_loss: 0.5336 - val_mae: 0.5304\n",
      "Epoch 248/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.6062 - mae: 0.5757 - val_loss: 0.5273 - val_mae: 0.5165\n",
      "Epoch 249/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.6684 - mae: 0.5917 - val_loss: 0.5480 - val_mae: 0.5502\n",
      "Epoch 250/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.6322 - mae: 0.5879 - val_loss: 0.5281 - val_mae: 0.5110\n",
      "Epoch 251/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.6215 - mae: 0.5686 - val_loss: 0.5248 - val_mae: 0.5020\n",
      "Epoch 252/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.6207 - mae: 0.5696 - val_loss: 0.5258 - val_mae: 0.5101\n",
      "Epoch 253/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.6372 - mae: 0.5988 - val_loss: 0.5319 - val_mae: 0.5245\n",
      "Epoch 254/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.6548 - mae: 0.5852 - val_loss: 0.5333 - val_mae: 0.5151\n",
      "Epoch 255/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.6723 - mae: 0.6160 - val_loss: 0.5278 - val_mae: 0.5204\n",
      "Epoch 256/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.6158 - mae: 0.5896 - val_loss: 0.5341 - val_mae: 0.5226\n",
      "Epoch 257/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.6710 - mae: 0.5904 - val_loss: 0.5222 - val_mae: 0.5023\n",
      "Epoch 258/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7067 - mae: 0.5974 - val_loss: 0.5196 - val_mae: 0.4887\n",
      "Epoch 259/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.6626 - mae: 0.5840 - val_loss: 0.5301 - val_mae: 0.4957\n",
      "Epoch 260/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7007 - mae: 0.6054 - val_loss: 0.5234 - val_mae: 0.4949\n",
      "Epoch 261/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.6362 - mae: 0.5689 - val_loss: 0.5202 - val_mae: 0.5040\n",
      "Epoch 262/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.5577 - mae: 0.5435 - val_loss: 0.5508 - val_mae: 0.5549\n",
      "Epoch 263/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.6213 - mae: 0.5816 - val_loss: 0.5193 - val_mae: 0.5114\n",
      "Epoch 264/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5645 - mae: 0.5430 - val_loss: 0.5229 - val_mae: 0.5133\n",
      "Epoch 265/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.6394 - mae: 0.5903 - val_loss: 0.5182 - val_mae: 0.4918\n",
      "Epoch 266/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.6175 - mae: 0.5684 - val_loss: 0.5223 - val_mae: 0.5137\n",
      "Epoch 267/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.6757 - mae: 0.6070 - val_loss: 0.5156 - val_mae: 0.4890\n",
      "Epoch 268/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.6880 - mae: 0.6017 - val_loss: 0.5192 - val_mae: 0.4961\n",
      "Epoch 269/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.6334 - mae: 0.5798 - val_loss: 0.5190 - val_mae: 0.5080\n",
      "Epoch 270/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.6255 - mae: 0.5850 - val_loss: 0.5261 - val_mae: 0.5129\n",
      "Epoch 271/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.5880 - mae: 0.5630 - val_loss: 0.5171 - val_mae: 0.4940\n",
      "Epoch 272/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.6778 - mae: 0.5848 - val_loss: 0.5147 - val_mae: 0.4852\n",
      "Epoch 273/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5782 - mae: 0.5542 - val_loss: 0.5390 - val_mae: 0.5471\n",
      "Epoch 274/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.6694 - mae: 0.6009 - val_loss: 0.5171 - val_mae: 0.4947\n",
      "Epoch 275/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.6016 - mae: 0.5767 - val_loss: 0.5156 - val_mae: 0.5062\n",
      "Epoch 276/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.6879 - mae: 0.5958 - val_loss: 0.5160 - val_mae: 0.4846\n",
      "Epoch 277/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.5725 - mae: 0.5665 - val_loss: 0.5205 - val_mae: 0.5181\n",
      "Epoch 278/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.5889 - mae: 0.5824 - val_loss: 0.5140 - val_mae: 0.4877\n",
      "Epoch 279/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.6735 - mae: 0.5752 - val_loss: 0.5190 - val_mae: 0.4816\n",
      "Epoch 280/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.5866 - mae: 0.5615 - val_loss: 0.5181 - val_mae: 0.5154\n",
      "Epoch 281/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.6378 - mae: 0.5885 - val_loss: 0.5195 - val_mae: 0.4800\n",
      "Epoch 282/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.6719 - mae: 0.5974 - val_loss: 0.5249 - val_mae: 0.5302\n",
      "Epoch 283/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5824 - mae: 0.5646 - val_loss: 0.5177 - val_mae: 0.5190\n",
      "Epoch 284/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.6356 - mae: 0.5873 - val_loss: 0.5154 - val_mae: 0.5020\n",
      "Epoch 285/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.6648 - mae: 0.5929 - val_loss: 0.5101 - val_mae: 0.4944\n",
      "Epoch 286/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.5764 - mae: 0.5609 - val_loss: 0.5120 - val_mae: 0.4780\n",
      "Epoch 287/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.6113 - mae: 0.5648 - val_loss: 0.5099 - val_mae: 0.4959\n",
      "Epoch 288/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5759 - mae: 0.5523 - val_loss: 0.5144 - val_mae: 0.4918\n",
      "Epoch 289/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.6572 - mae: 0.5929 - val_loss: 0.5238 - val_mae: 0.5207\n",
      "Epoch 290/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.6585 - mae: 0.5833 - val_loss: 0.5126 - val_mae: 0.5137\n",
      "Epoch 291/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5767 - mae: 0.5603 - val_loss: 0.5098 - val_mae: 0.4971\n",
      "Epoch 292/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5622 - mae: 0.5373 - val_loss: 0.5135 - val_mae: 0.5122\n",
      "Epoch 293/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5567 - mae: 0.5563 - val_loss: 0.5220 - val_mae: 0.5287\n",
      "Epoch 294/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.6143 - mae: 0.5749 - val_loss: 0.5149 - val_mae: 0.4990\n",
      "Epoch 295/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.6143 - mae: 0.5617 - val_loss: 0.5111 - val_mae: 0.5072\n",
      "Epoch 296/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5934 - mae: 0.5611 - val_loss: 0.5095 - val_mae: 0.4998\n",
      "Epoch 297/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5606 - mae: 0.5475 - val_loss: 0.5170 - val_mae: 0.5212\n",
      "Epoch 298/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.5818 - mae: 0.5779 - val_loss: 0.5080 - val_mae: 0.5056\n",
      "Epoch 299/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5877 - mae: 0.5673 - val_loss: 0.5293 - val_mae: 0.5467\n",
      "Epoch 300/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.6736 - mae: 0.6048 - val_loss: 0.5100 - val_mae: 0.5051\n",
      "Epoch 301/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.7349 - mae: 0.6303 - val_loss: 0.5097 - val_mae: 0.4841\n",
      "Epoch 302/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.6612 - mae: 0.6032 - val_loss: 0.5201 - val_mae: 0.4788\n",
      "Epoch 303/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.6344 - mae: 0.5678 - val_loss: 0.5094 - val_mae: 0.4939\n",
      "Epoch 304/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.6763 - mae: 0.6177 - val_loss: 0.5074 - val_mae: 0.5001\n",
      "Epoch 305/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5047 - mae: 0.5319 - val_loss: 0.5066 - val_mae: 0.5022\n",
      "Epoch 306/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.5976 - mae: 0.5784 - val_loss: 0.5058 - val_mae: 0.5030\n",
      "Epoch 307/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5964 - mae: 0.5669 - val_loss: 0.5068 - val_mae: 0.4839\n",
      "Epoch 308/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5562 - mae: 0.5364 - val_loss: 0.5045 - val_mae: 0.4964\n",
      "Epoch 309/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.6785 - mae: 0.5831 - val_loss: 0.5045 - val_mae: 0.5000\n",
      "Epoch 310/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.7271 - mae: 0.5974 - val_loss: 0.5075 - val_mae: 0.5111\n",
      "Epoch 311/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5587 - mae: 0.5476 - val_loss: 0.5043 - val_mae: 0.5083\n",
      "Epoch 312/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.6121 - mae: 0.5729 - val_loss: 0.5064 - val_mae: 0.4869\n",
      "Epoch 313/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.6560 - mae: 0.5864 - val_loss: 0.5125 - val_mae: 0.4743\n",
      "Epoch 314/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.6366 - mae: 0.5760 - val_loss: 0.5042 - val_mae: 0.4985\n",
      "Epoch 315/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5059 - mae: 0.5153 - val_loss: 0.5097 - val_mae: 0.5176\n",
      "Epoch 316/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.6250 - mae: 0.5717 - val_loss: 0.5039 - val_mae: 0.4985\n",
      "Epoch 317/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.6084 - mae: 0.5804 - val_loss: 0.5038 - val_mae: 0.4914\n",
      "Epoch 318/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.5638 - mae: 0.5550 - val_loss: 0.5057 - val_mae: 0.5078\n",
      "Epoch 319/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5905 - mae: 0.5632 - val_loss: 0.5068 - val_mae: 0.4776\n",
      "Epoch 320/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.6667 - mae: 0.5940 - val_loss: 0.4990 - val_mae: 0.4913\n",
      "Epoch 321/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5729 - mae: 0.5555 - val_loss: 0.5041 - val_mae: 0.4898\n",
      "Epoch 322/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5963 - mae: 0.5842 - val_loss: 0.5113 - val_mae: 0.5195\n",
      "Epoch 323/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.6534 - mae: 0.5945 - val_loss: 0.5046 - val_mae: 0.4902\n",
      "Epoch 324/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5498 - mae: 0.5445 - val_loss: 0.4974 - val_mae: 0.4838\n",
      "Epoch 325/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5600 - mae: 0.5417 - val_loss: 0.4961 - val_mae: 0.4819\n",
      "Epoch 326/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5689 - mae: 0.5403 - val_loss: 0.4961 - val_mae: 0.4891\n",
      "Epoch 327/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.6714 - mae: 0.5989 - val_loss: 0.5040 - val_mae: 0.4979\n",
      "Epoch 328/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5634 - mae: 0.5579 - val_loss: 0.5029 - val_mae: 0.4854\n",
      "Epoch 329/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5610 - mae: 0.5255 - val_loss: 0.5036 - val_mae: 0.5063\n",
      "Epoch 330/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.5958 - mae: 0.5620 - val_loss: 0.5153 - val_mae: 0.5135\n",
      "Epoch 331/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.6450 - mae: 0.5987 - val_loss: 0.4968 - val_mae: 0.4976\n",
      "Epoch 332/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.6822 - mae: 0.6035 - val_loss: 0.5003 - val_mae: 0.4779\n",
      "Epoch 333/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.6138 - mae: 0.5809 - val_loss: 0.4949 - val_mae: 0.4925\n",
      "Epoch 334/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4968 - mae: 0.5209 - val_loss: 0.4928 - val_mae: 0.4943\n",
      "Epoch 335/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.6293 - mae: 0.5616 - val_loss: 0.4935 - val_mae: 0.4918\n",
      "Epoch 336/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.6541 - mae: 0.5857 - val_loss: 0.4931 - val_mae: 0.4888\n",
      "Epoch 337/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.5969 - mae: 0.5782 - val_loss: 0.4923 - val_mae: 0.4690\n",
      "Epoch 338/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.6720 - mae: 0.5803 - val_loss: 0.5021 - val_mae: 0.4724\n",
      "Epoch 339/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.6423 - mae: 0.5654 - val_loss: 0.4972 - val_mae: 0.5012\n",
      "Epoch 340/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.5827 - mae: 0.5522 - val_loss: 0.4922 - val_mae: 0.4928\n",
      "Epoch 341/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.6925 - mae: 0.6092 - val_loss: 0.4977 - val_mae: 0.4763\n",
      "Epoch 342/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.5549 - mae: 0.5345 - val_loss: 0.4928 - val_mae: 0.4809\n",
      "Epoch 343/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.6849 - mae: 0.5814 - val_loss: 0.4921 - val_mae: 0.4820\n",
      "Epoch 344/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.6228 - mae: 0.5752 - val_loss: 0.4970 - val_mae: 0.5081\n",
      "Epoch 345/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.6201 - mae: 0.5796 - val_loss: 0.4929 - val_mae: 0.4731\n",
      "Epoch 346/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.6536 - mae: 0.6021 - val_loss: 0.4890 - val_mae: 0.4878\n",
      "Epoch 347/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.6483 - mae: 0.5983 - val_loss: 0.5036 - val_mae: 0.5219\n",
      "Epoch 348/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.6312 - mae: 0.5693 - val_loss: 0.4899 - val_mae: 0.4880\n",
      "Epoch 349/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5948 - mae: 0.5685 - val_loss: 0.4904 - val_mae: 0.4796\n",
      "Epoch 350/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.6945 - mae: 0.6030 - val_loss: 0.4853 - val_mae: 0.4762\n",
      "Epoch 351/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.6951 - mae: 0.5956 - val_loss: 0.4889 - val_mae: 0.4895\n",
      "Epoch 352/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.6570 - mae: 0.5788 - val_loss: 0.4941 - val_mae: 0.4964\n",
      "Epoch 353/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.6162 - mae: 0.5789 - val_loss: 0.4996 - val_mae: 0.5197\n",
      "Epoch 354/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5986 - mae: 0.5782 - val_loss: 0.4871 - val_mae: 0.4866\n",
      "Epoch 355/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.6520 - mae: 0.5994 - val_loss: 0.4884 - val_mae: 0.4822\n",
      "Epoch 356/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.5628 - mae: 0.5396 - val_loss: 0.4861 - val_mae: 0.4865\n",
      "Epoch 357/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5984 - mae: 0.5472 - val_loss: 0.4898 - val_mae: 0.4971\n",
      "Epoch 358/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.6221 - mae: 0.5747 - val_loss: 0.4910 - val_mae: 0.4827\n",
      "Epoch 359/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.6049 - mae: 0.5534 - val_loss: 0.4906 - val_mae: 0.5015\n",
      "Epoch 360/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5639 - mae: 0.5684 - val_loss: 0.4856 - val_mae: 0.4691\n",
      "Epoch 361/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.6043 - mae: 0.5739 - val_loss: 0.4879 - val_mae: 0.4907\n",
      "Epoch 362/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5866 - mae: 0.5600 - val_loss: 0.5238 - val_mae: 0.5563\n",
      "Epoch 363/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.6257 - mae: 0.5863 - val_loss: 0.5059 - val_mae: 0.5235\n",
      "Epoch 364/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.6856 - mae: 0.6123 - val_loss: 0.4851 - val_mae: 0.4914\n",
      "Epoch 365/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.6090 - mae: 0.5759 - val_loss: 0.4848 - val_mae: 0.4829\n",
      "Epoch 366/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5902 - mae: 0.5746 - val_loss: 0.4840 - val_mae: 0.4856\n",
      "Epoch 367/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5682 - mae: 0.5490 - val_loss: 0.4781 - val_mae: 0.4772\n",
      "Epoch 368/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5844 - mae: 0.5582 - val_loss: 0.4846 - val_mae: 0.4962\n",
      "Epoch 369/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.6014 - mae: 0.5718 - val_loss: 0.4836 - val_mae: 0.4931\n",
      "Epoch 370/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5456 - mae: 0.5497 - val_loss: 0.4811 - val_mae: 0.4816\n",
      "Epoch 371/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.6557 - mae: 0.5780 - val_loss: 0.4773 - val_mae: 0.4739\n",
      "Epoch 372/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.6112 - mae: 0.5616 - val_loss: 0.4912 - val_mae: 0.5141\n",
      "Epoch 373/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5838 - mae: 0.5733 - val_loss: 0.4818 - val_mae: 0.4851\n",
      "Epoch 374/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.6646 - mae: 0.5881 - val_loss: 0.4782 - val_mae: 0.4768\n",
      "Epoch 375/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.6141 - mae: 0.5555 - val_loss: 0.4813 - val_mae: 0.4642\n",
      "Epoch 376/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.6091 - mae: 0.5579 - val_loss: 0.4811 - val_mae: 0.4907\n",
      "Epoch 377/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.5683 - mae: 0.5481 - val_loss: 0.4905 - val_mae: 0.5038\n",
      "Epoch 378/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.6092 - mae: 0.5579 - val_loss: 0.4792 - val_mae: 0.4892\n",
      "Epoch 379/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.6065 - mae: 0.5781 - val_loss: 0.4751 - val_mae: 0.4751\n",
      "Epoch 380/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5769 - mae: 0.5370 - val_loss: 0.4735 - val_mae: 0.4743\n",
      "Epoch 381/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7546 - mae: 0.6200 - val_loss: 0.4868 - val_mae: 0.4663\n",
      "Epoch 382/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5830 - mae: 0.5436 - val_loss: 0.4943 - val_mae: 0.5149\n",
      "Epoch 383/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.6597 - mae: 0.5831 - val_loss: 0.4775 - val_mae: 0.4670\n",
      "Epoch 384/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5452 - mae: 0.5341 - val_loss: 0.4795 - val_mae: 0.4881\n",
      "Epoch 385/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5374 - mae: 0.5260 - val_loss: 0.4943 - val_mae: 0.5212\n",
      "Epoch 386/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.5725 - mae: 0.5672 - val_loss: 0.4805 - val_mae: 0.4679\n",
      "Epoch 387/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5860 - mae: 0.5642 - val_loss: 0.4853 - val_mae: 0.5029\n",
      "Epoch 388/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5610 - mae: 0.5350 - val_loss: 0.4761 - val_mae: 0.4720\n",
      "Epoch 389/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.5692 - mae: 0.5326 - val_loss: 0.4795 - val_mae: 0.4926\n",
      "Epoch 390/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5970 - mae: 0.5585 - val_loss: 0.4819 - val_mae: 0.4661\n",
      "Epoch 391/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5652 - mae: 0.5313 - val_loss: 0.4827 - val_mae: 0.4990\n",
      "Epoch 392/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.5756 - mae: 0.5677 - val_loss: 0.4754 - val_mae: 0.4808\n",
      "Epoch 393/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.6307 - mae: 0.5991 - val_loss: 0.4772 - val_mae: 0.4879\n",
      "Epoch 394/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.6429 - mae: 0.5673 - val_loss: 0.4758 - val_mae: 0.4800\n",
      "Epoch 395/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.6042 - mae: 0.5737 - val_loss: 0.4817 - val_mae: 0.4980\n",
      "Epoch 396/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.5543 - mae: 0.5632 - val_loss: 0.4836 - val_mae: 0.5064\n",
      "Epoch 397/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.6623 - mae: 0.5915 - val_loss: 0.4682 - val_mae: 0.4623\n",
      "Epoch 398/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5171 - mae: 0.5293 - val_loss: 0.4697 - val_mae: 0.4637\n",
      "Epoch 399/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5854 - mae: 0.5511 - val_loss: 0.4705 - val_mae: 0.4842\n",
      "Epoch 400/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.6026 - mae: 0.5687 - val_loss: 0.4739 - val_mae: 0.4780\n",
      "Epoch 401/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4807 - mae: 0.5096 - val_loss: 0.4693 - val_mae: 0.4784\n",
      "Epoch 402/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.5668 - mae: 0.5468 - val_loss: 0.4803 - val_mae: 0.4996\n",
      "Epoch 403/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5944 - mae: 0.5501 - val_loss: 0.4944 - val_mae: 0.5253\n",
      "Epoch 404/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.6712 - mae: 0.6084 - val_loss: 0.4705 - val_mae: 0.4766\n",
      "Epoch 405/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.6810 - mae: 0.5975 - val_loss: 0.4887 - val_mae: 0.4653\n",
      "Epoch 406/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.6085 - mae: 0.5639 - val_loss: 0.4693 - val_mae: 0.4849\n",
      "Epoch 407/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5941 - mae: 0.5611 - val_loss: 0.4660 - val_mae: 0.4829\n",
      "Epoch 408/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.6042 - mae: 0.5606 - val_loss: 0.4670 - val_mae: 0.4777\n",
      "Epoch 409/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.6201 - mae: 0.5737 - val_loss: 0.4664 - val_mae: 0.4659\n",
      "Epoch 410/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.5811 - mae: 0.5600 - val_loss: 0.4644 - val_mae: 0.4656\n",
      "Epoch 411/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.5573 - mae: 0.5371 - val_loss: 0.4858 - val_mae: 0.5146\n",
      "Epoch 412/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5850 - mae: 0.5663 - val_loss: 0.4685 - val_mae: 0.4701\n",
      "Epoch 413/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.6186 - mae: 0.5750 - val_loss: 0.4657 - val_mae: 0.4646\n",
      "Epoch 414/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5798 - mae: 0.5496 - val_loss: 0.4773 - val_mae: 0.4610\n",
      "Epoch 415/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.6572 - mae: 0.5873 - val_loss: 0.4714 - val_mae: 0.4612\n",
      "Epoch 416/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5548 - mae: 0.5470 - val_loss: 0.4712 - val_mae: 0.4855\n",
      "Epoch 417/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5076 - mae: 0.5245 - val_loss: 0.4851 - val_mae: 0.5204\n",
      "Epoch 418/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.5323 - mae: 0.5582 - val_loss: 0.4651 - val_mae: 0.4797\n",
      "Epoch 419/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5573 - mae: 0.5177 - val_loss: 0.4713 - val_mae: 0.4843\n",
      "Epoch 420/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.5658 - mae: 0.5497 - val_loss: 0.4909 - val_mae: 0.5232\n",
      "Epoch 421/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.5514 - mae: 0.5582 - val_loss: 0.4699 - val_mae: 0.4813\n",
      "Epoch 422/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.6088 - mae: 0.5678 - val_loss: 0.4622 - val_mae: 0.4672\n",
      "Epoch 423/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.5019 - mae: 0.5202 - val_loss: 0.4774 - val_mae: 0.5019\n",
      "Epoch 424/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5595 - mae: 0.5544 - val_loss: 0.4877 - val_mae: 0.5131\n",
      "Epoch 425/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.6055 - mae: 0.5758 - val_loss: 0.4808 - val_mae: 0.5054\n",
      "Epoch 426/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5933 - mae: 0.5570 - val_loss: 0.4624 - val_mae: 0.4738\n",
      "Epoch 427/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5321 - mae: 0.5208 - val_loss: 0.4723 - val_mae: 0.4935\n",
      "Epoch 428/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5382 - mae: 0.5455 - val_loss: 0.4743 - val_mae: 0.4973\n",
      "Epoch 429/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5638 - mae: 0.5581 - val_loss: 0.4984 - val_mae: 0.5306\n",
      "Epoch 430/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.5971 - mae: 0.5741 - val_loss: 0.4755 - val_mae: 0.4823\n",
      "Epoch 431/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.5885 - mae: 0.5440 - val_loss: 0.4684 - val_mae: 0.4797\n",
      "Epoch 432/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.5604 - mae: 0.5446 - val_loss: 0.4632 - val_mae: 0.4781\n",
      "Epoch 433/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5886 - mae: 0.5561 - val_loss: 0.4589 - val_mae: 0.4724\n",
      "Epoch 434/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5295 - mae: 0.5295 - val_loss: 0.4791 - val_mae: 0.5123\n",
      "Epoch 435/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.5867 - mae: 0.5733 - val_loss: 0.4610 - val_mae: 0.4785\n",
      "Epoch 436/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5462 - mae: 0.5322 - val_loss: 0.4599 - val_mae: 0.4681\n",
      "Epoch 437/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.6428 - mae: 0.5753 - val_loss: 0.4562 - val_mae: 0.4603\n",
      "Epoch 438/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.6326 - mae: 0.5710 - val_loss: 0.4559 - val_mae: 0.4661\n",
      "Epoch 439/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.6686 - mae: 0.5823 - val_loss: 0.4660 - val_mae: 0.4933\n",
      "Epoch 440/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5715 - mae: 0.5545 - val_loss: 0.4623 - val_mae: 0.4798\n",
      "Epoch 441/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5586 - mae: 0.5451 - val_loss: 0.4574 - val_mae: 0.4664\n",
      "Epoch 442/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5754 - mae: 0.5238 - val_loss: 0.4586 - val_mae: 0.4784\n",
      "Epoch 443/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5212 - mae: 0.5203 - val_loss: 0.4593 - val_mae: 0.4755\n",
      "Epoch 444/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5716 - mae: 0.5404 - val_loss: 0.4717 - val_mae: 0.4704\n",
      "Epoch 445/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.6041 - mae: 0.5528 - val_loss: 0.4945 - val_mae: 0.5331\n",
      "Epoch 446/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.6391 - mae: 0.5849 - val_loss: 0.4595 - val_mae: 0.4790\n",
      "Epoch 447/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5655 - mae: 0.5607 - val_loss: 0.4533 - val_mae: 0.4635\n",
      "Epoch 448/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.5597 - mae: 0.5302 - val_loss: 0.4584 - val_mae: 0.4901\n",
      "Epoch 449/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.5454 - mae: 0.5400 - val_loss: 0.4497 - val_mae: 0.4578\n",
      "Epoch 450/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.6250 - mae: 0.5694 - val_loss: 0.4641 - val_mae: 0.4904\n",
      "Epoch 451/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.5897 - mae: 0.5475 - val_loss: 0.4563 - val_mae: 0.4727\n",
      "Epoch 452/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5656 - mae: 0.5445 - val_loss: 0.4709 - val_mae: 0.5000\n",
      "Epoch 453/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5831 - mae: 0.5532 - val_loss: 0.4570 - val_mae: 0.4879\n",
      "Epoch 454/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5455 - mae: 0.5483 - val_loss: 0.4607 - val_mae: 0.4531\n",
      "Epoch 455/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.6654 - mae: 0.5768 - val_loss: 0.4593 - val_mae: 0.4537\n",
      "Epoch 456/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.5787 - mae: 0.5444 - val_loss: 0.4531 - val_mae: 0.4693\n",
      "Epoch 457/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.6293 - mae: 0.5728 - val_loss: 0.4723 - val_mae: 0.4604\n",
      "Epoch 458/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.6552 - mae: 0.5677 - val_loss: 0.4552 - val_mae: 0.4636\n",
      "Epoch 459/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5565 - mae: 0.5460 - val_loss: 0.4526 - val_mae: 0.4621\n",
      "Epoch 460/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.6012 - mae: 0.5631 - val_loss: 0.4584 - val_mae: 0.4712\n",
      "Epoch 461/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5474 - mae: 0.5237 - val_loss: 0.4536 - val_mae: 0.4619\n",
      "Epoch 462/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5698 - mae: 0.5437 - val_loss: 0.4555 - val_mae: 0.4556\n",
      "Epoch 463/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.6125 - mae: 0.5625 - val_loss: 0.4599 - val_mae: 0.4801\n",
      "Epoch 464/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.6154 - mae: 0.5830 - val_loss: 0.4543 - val_mae: 0.4602\n",
      "Epoch 465/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5761 - mae: 0.5336 - val_loss: 0.4631 - val_mae: 0.4861\n",
      "Epoch 466/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5466 - mae: 0.5400 - val_loss: 0.4673 - val_mae: 0.4609\n",
      "Epoch 467/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5758 - mae: 0.5452 - val_loss: 0.4588 - val_mae: 0.4808\n",
      "Epoch 468/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.5489 - mae: 0.5429 - val_loss: 0.4563 - val_mae: 0.4722\n",
      "Epoch 469/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.6004 - mae: 0.5506 - val_loss: 0.4552 - val_mae: 0.4731\n",
      "Epoch 470/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5739 - mae: 0.5626 - val_loss: 0.4628 - val_mae: 0.4881\n",
      "Epoch 471/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5721 - mae: 0.5519 - val_loss: 0.4556 - val_mae: 0.4765\n",
      "Epoch 472/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5769 - mae: 0.5569 - val_loss: 0.4602 - val_mae: 0.4812\n",
      "Epoch 473/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5570 - mae: 0.5358 - val_loss: 0.4827 - val_mae: 0.5180\n",
      "Epoch 474/1000\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.5410 - mae: 0.5198 - val_loss: 0.4601 - val_mae: 0.4726\n",
      "Epoch 474: early stopping\n",
      "Restoring model weights from the end of the best epoch: 449.\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "history = model.fit(X_train,\n",
    "                    y_train,\n",
    "                    epochs=1000,\n",
    "                    batch_size=32,\n",
    "                    validation_data= [X_val,y_val],\n",
    "                    callbacks=[es, mc]\n",
    "                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 - 0s - 1ms/step - loss: 0.4497 - mae: 0.4578\n",
      "Testing accuracy 0.45784100890159607\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(X_test,y_test, verbose=2)\n",
    "print(f\"Testing accuracy {test_acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quick test to see model preformance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "Prediction: 3.0957723\n",
      "Actual value: 2.730769\n"
     ]
    }
   ],
   "source": [
    "random_index = np.random.randint(0, len(X_test))\n",
    "sample = X_test.iloc[[random_index]]\n",
    "\n",
    "prediction = model.predict(sample)\n",
    "\n",
    "print(\"Prediction:\", prediction[0][0])\n",
    "print(\"Actual value:\",y_test.iloc[random_index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "# saving in 2 formats\n",
    "model.save(\"./data/model_2a.keras\")\n",
    "model.save(\"./data/model_2a.h5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
